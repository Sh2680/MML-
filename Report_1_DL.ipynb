{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjFfJ0m2V+elCmbQrKAZ3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sh2680/MML-/blob/main/Report_1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEEP LEARNING**\n",
        "\n",
        "Deep learning is a subset of machine learning and artificial intelligence (AI) that focuses on using neural networks with many layers  to model and understand complex patterns in data. These neural networks are designed to simulate the way the human brain works, enabling computers to perform tasks like image and speech recognition, natural language processing, and decision making.\n",
        "\n",
        "\n",
        "**Neural Networks**:  The core of deep learning is the neural network, which consists of layers of interconnected nodes or neurons. Each neuron processes input data and passes the result to the next layer.\n",
        "\n",
        "**PyTorch**  :\n",
        "It is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Meta AI Research.It is optimized for GPU-accelerated computing and deep neural networks.\n",
        "PyTorch has a modular architecture that allows users to build neural networks easily\n",
        "\n",
        "**Bias**:  The difference between the expected value of the model's predictions and the true value. High bias means the model is too simple and does not capture the true relationship in the data.\n",
        "\n",
        "**Variance**:  The amount of variation in the model's predictions due to different training data. High variance means the model is too complex and overfits the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "6N1spyqI_LrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Basic MLP (Multilayer Perceptron)**\n",
        "\n",
        "A basic MLP is a neural network with multiple layers of neurons. Each layer is fully connected to the next layer. The output of each layer is fed into the next layer through a non-linear activation function."
      ],
      "metadata": {
        "id": "eRy5Kzi_Yl1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D7VdEigK_HJ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_size = 784  # Example for MNIST dataset\n",
        "hidden_size = 128\n",
        "output_size = 10  # Number of classes in MNIST\n",
        "\n",
        "model = MLP(input_size, hidden_size, output_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Deep Neural Nets**\n",
        "\n",
        "Deep neural networks are composed of multiple hidden layers. Each layer is designed to learn complex representations of the data."
      ],
      "metadata": {
        "id": "yN24ct-TY_TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)  # Input layer -> Hidden layer 1\n",
        "        self.fc2 = nn.Linear(256, 128)  # Hidden layer 1 -> Hidden layer 2\n",
        "        self.fc3 = nn.Linear(128, 10)  # Hidden layer 2 -> Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = DeepNet()"
      ],
      "metadata": {
        "id": "-UqL7hzXZkqO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(DeepNN, self).__init__()\n",
        "        layers = []\n",
        "        last_size = input_size\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(last_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_size = hidden_size\n",
        "        layers.append(nn.Linear(last_size, output_size))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "hidden_sizes = [256, 128, 64]\n",
        "model = DeepNN(input_size, hidden_sizes, output_size)\n"
      ],
      "metadata": {
        "id": "1ILn4Z_1Z9My"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Back Propagation**\n",
        "\n",
        "Backpropagation is an algorithm used to update the weights of a neural network during training. It involves computing the partial derivatives of the loss function with respect to each weight and then updating the weights based on these derivatives.\n",
        "python"
      ],
      "metadata": {
        "id": "v9SajHE9Zpm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Example training loop\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(torch.randn(64, input_size))  # Example batch\n",
        "    loss = criterion(outputs, torch.randint(0, output_size, (64,)))\n",
        "    loss.backward() # compute the gradient\n",
        "    optimizer.step() # update the weights\n"
      ],
      "metadata": {
        "id": "NxVCQ_2iZwtu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Loss Functions**\n",
        "A loss function measures how well the model's predictions match the actual data. Examples include Mean Squared Error (MSE) for regression and Cross Entropy Loss for classification."
      ],
      "metadata": {
        "id": "vajh8a3YaFhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()  # For classification\n"
      ],
      "metadata": {
        "id": "I-51KLteaNIC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # Mean Squared Error (for regression)"
      ],
      "metadata": {
        "id": "H8no6hmcazWO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Activation Functions**\n",
        "\n",
        "Activation functions introduce non-linearities into the model. Common examples include ReLU, Sigmoid, and Tanh."
      ],
      "metadata": {
        "id": "n_7OoKU1aToz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_fn = nn.ReLU()  # Example activation function\n"
      ],
      "metadata": {
        "id": "92kyFRmUaR8H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Regularization**\n",
        "\n",
        "Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function.\n",
        " Common regularization techniques include:\n",
        "\n",
        "L1 Regularization (Lasso)\n",
        "\n",
        "L2 Regularization (Ridge)\n",
        "\n",
        "Dropout"
      ],
      "metadata": {
        "id": "eaSZQx8WbNp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability\n",
        "\n",
        "# L2 regularization is typically implemented through the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)  # L2 regularization\n"
      ],
      "metadata": {
        "id": "lj4uBsIEbiz7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "CNNs are designed to process data with spatial relationships, such as images. They consist of convolutional and pooling layers."
      ],
      "metadata": {
        "id": "WLo5I_8dbu_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(64*12*12, 128)  # Assuming input size is 28x28 (MNIST)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = x.view(-1, 64*12*12)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n"
      ],
      "metadata": {
        "id": "pxUQ2p4qb86E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "RNNs are designed to process sequential data, such as time series or text. They consist of recurrent layers."
      ],
      "metadata": {
        "id": "uTI-J3fbcKUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), hidden_size)  # Initial hidden state\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Get last output\n",
        "        return out\n",
        "\n",
        "input_size = 28  # Example for sequence data\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n"
      ],
      "metadata": {
        "id": "jp2752IFcJRB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Transformers**\n",
        "\n",
        "Transformers are a type of neural network designed for natural language processing tasks. They consist of self-attention layers and feed-forward networks."
      ],
      "metadata": {
        "id": "tm4AZZPAcog4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=512, nhead=8), num_layers=6)\n",
        "        self.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.encoder(src)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "model = Transformer()\n"
      ],
      "metadata": {
        "id": "sh5YgUPkcwwF"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}